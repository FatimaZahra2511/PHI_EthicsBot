chapter;theme;claim;quote;page_ref;design_guideline
Ch.1 - The Infosphere;digital environment;Floridi defines the infosphere as the total informational environment in which all agents‚Äîhuman and artificial‚Äîinteract.;IE offers an informational, network-based conception of the environment, which now includes both natural and artificial ecosystems.;p.279‚Äì280;Design AI systems as respectful participants within the shared infosphere.
Ch.2 - Moral Agency;moral agency;"Moral action is distributed across humans and artifacts; responsibility is shared.";The use of LoAs enables one to distinguish between accountability and responsibility.;p.158‚Äì159;Implement traceable decision logs to preserve accountability across human and AI actors.
Ch.3 - Information Ethics;intrinsic value;Information has intrinsic moral worth, not only instrumental value.;IE is an environmental ethics based on information/infosphere/entropy rather than life/ecosystem/pain.;p.98‚Äì111;Avoid data destruction or manipulation that erodes informational integrity.
Ch.4 - Privacy;privacy;Privacy sustains personal identity within the infosphere and protects informational autonomy.;'Privacy is the infrastructure of personal identity.';p.243‚Äì247;Default to data-minimization and contextual integrity principles.
Ch.5 - Transparency;transparency;Opacity in algorithmic systems weakens trust and accountability.;Promoting prescriptive action is reasonable even when there is no responsibility but only moral accountability.;p.158‚Äì159;Ensure explainability proportional to the system‚Äôs social impact.
Ch.6 - Harm;informational harm;Information harms can be intangible yet morally significant.;Increase in its level of entropy and hence an instance of evil.;p.98;Anticipate and mitigate secondary inference harms in datasets.
Ch.7 - Responsibility;distributed responsibility;"In complex systems, no single agent bears all moral weight; design must assign roles clearly.";Big issues call for big agents.;p.274‚Äì276;Define explicit accountability maps between human and automated decisions.
Ch.8 - Consent;consent;Consent must be informed, revocable, and context-aware in digital interactions.;The right to informational privacy shields one‚Äôs personal identity.;p.243‚Äì247;Implement granular, revocable consent settings.
Ch.9 - Environmental Impact;eco-ethics;Digital ethics extends to environmental effects of information processes.;'The infosphere includes the natural environment as an informational ecosystem.';p.279‚Äì280;Optimize energy usage and lifecycle of AI systems.
Ch.10 - Design Ethics;ethical design;Ethical constraints are design parameters, not afterthoughts.;'Ethics must be built into the architecture of the infosphere.';p.274‚Äì275;Integrate fairness, transparency, and privacy during requirement analysis.
Ch.11 - Education;digital literacy;Ethical competence requires digital literacy and critical reflection.;IE strives to provide a good, unbiased platform from which to educate‚Ä¶ the citizens of an information society.;p.132;Promote ethical-AI training for developers and users alike.
Ch.12 - Future of Ethics;AI governance;Floridi envisions governance as coordination of human and artificial moral agents.;'Ethical governance will be multi-agent and systemic.';p.274‚Äì276;Adopt governance models combining human oversight with AI auditing.
Ch.1 ‚Äî Ethics after the information revolution;infosphere (definition);"The infosphere is the whole informational environment of entities, properties, interactions, and relations; not just cyberspace.";Minimally, it denotes the whole informational environment constituted by all informational entities‚Ä¶;p.6;Treat AI as a participant in a shared informational ecosystem, not just a tool.
Ch.12 ‚Äî Informational privacy;ontological friction ‚Üí privacy;Informational privacy varies with the 'ontological friction' that resists information flow in the infosphere.;Informational privacy is a function of the ontological friction in the infosphere.;p.231‚Äì232;Increase friction (rate limits, minimization, local processing) when data sensitivity is high.
Ch.7 ‚Äî Morality of artificial agents;moral agency of AAs;Artificial agents can qualify as moral agents at appropriate levels of abstraction and be accountable sources of moral action.;The use of LoAs enables one to distinguish between accountability and responsibility.;p.158‚Äì159;Give agents accountability hooks: logs, audit trails, escalation pathways.
Ch.6 ‚Äî Intrinsic value of the infosphere;ontocentric axiology;Information ethics adopts an ontocentric view: informational entities may have intrinsic moral worth.;IE replaces biosphere with infosphere‚Ä¶ Not only inanimate but also ideal, intangible, or intellectual entities can have a minimal degree of moral value.;p.110‚Äì111;Preserve informational integrity: avoid unnecessary deletion, distortion, or entropy-creating transformations. 
Ch.1 ‚Äî Ethics after the information revolution;zettabyte era & risks;ICTs create unprecedented benefits and risks, ushering a ‚Äòzettabyte era‚Äô with ethical dilemmas (e.g., identity theft, divides).;We are living in the age of the zettabyte‚Ä¶ ICTs also carry significant risks and generate dilemmas‚Ä¶;p.5;Assess secondary effects (misuse, divides) during requirements and mitigate proactively.
Ch.12 ‚Äî Privacy & identity;privacy as identity construction;Protecting informational privacy preserves the freedom to (re)construct one‚Äôs identity.;Personal identity also depends on informational privacy.;p.243‚Äì247;Support deletion, reset, and pseudonymity to allow identity renegotiation.
Ch.12 ‚Äî Privacy & biometrics;labels vs. ontic data;"Identity theft thrives on detachable labels; robust identity should rely on ontic data and careful scope.";Arbitrary data‚Ä¶ are merely associated with someone‚Äôs identity and can easily be detached from it‚Ä¶ Enter biometrics.;p.247;"Avoid over-reliance on detachable identifiers; prefer strong binding with minimal exposure."
Ch.13 ‚Äî Distributed morality;DM in multi‚Äëagent systems;Morally significant outcomes can emerge from interactions of agents whose individual actions are morally negligible.;‚ÄòDistributed morality‚Äô‚Ä¶ refers to cases of moral actions that are the result of otherwise morally neutral‚Ä¶ interactions among agents constituting a multi‚Äëagent system.;p.262;Model system‚Äëlevel moral effects (feedback loops, emergent harms), not just component behavior.
Ch.1 ‚Äî Re‚Äëontologizing the infosphere;Turing insight & friction;Digital tools and digital objects share the same ontology, eroding friction and transforming interactions into read/write/execute.;In the re‚Äëontologized infosphere‚Ä¶ there is no ontological difference between processors and processed‚Ä¶ interactions become equally digital.;p.6;"Expect rapid propagation; add circuit‚Äëbreakers and throttles to prevent harm at scale."
Ch.8 ‚Äî Limits of virtue ethics;constructionism beyond the self;"Virtue ethics alone cannot scale to the global infosphere; constructionism must extend to environments and patients.";The kind of ethical constructionism needed today goes well beyond the education of the self‚Ä¶ It must also address‚Ä¶ the kind of global realities that are being built.;p.166‚Äì167;Evaluate system-level construction: what kinds of worlds your system builds and sustains.
Ch.13 ‚Äî Distributed Morality & Infraethics;infraethics (definition);Infraethics = ethical infrastructure: implicit expectations and practices (trust, respect, privacy, transparency) that enable moral behaviour at scale.;The morally good behaviour of a whole population of agents is also a matter of ‚Äòethical infrastructure‚Äô or infraethics.;p.271‚Äì272;Engineer moral enablers into products (defaults, norms, affordances) to steer behaviour toward good outcomes.
Ch.13 ‚Äî Infraethics as design;facilitators ensemble;"Infraethics must be designed, coordinated, and implemented; no single component suffices.";It is a whole ensemble of facilitators that need to be designed, coordinated, and implemented for an infraethics to become possible.;p.274‚Äì275;"Align policy, UX, governance, and tooling; measure uptake, not just features."
Ch.13 ‚Äî DM scope;DM becomes salient with ICT;"ICTs make distributed morality more common and influential; small neutral actions can scale into significant moral outcomes.";Instances of DM‚Ä¶ now play an increasingly important role‚Ä¶ and will be more and more influential in the future.;p.274‚Äì275;Model emergent effects (feedback loops, network harms) in risk assessments.
Ch.13 ‚Äî Global negotiation;global info ethics;"Information societies must negotiate with alternative, pre-existing moral traditions; infraethics needs contextual governance.";One may need to consider the global nature of information societies and the necessity to negotiate interactions with alternative‚Ä¶ traditions.;p.274‚Äì275;"Provide global defaults with local overrides; document value trade-offs."
Ch.7 ‚Äî Artificial Agents as Moral Agents;mindless morality;Artificial agents can be accountable sources of moral action without free will or mental states (‚Äòmindless morality‚Äô).;AAs‚Ä¶ can be fully accountable sources of moral action‚Ä¶ ‚Äòmindless morality‚Äô.;p.134‚Äì146;Implement accountability hooks: decision logging, audit trails, escalation paths.
Ch.6 ‚Äî Patient-Oriented Macroethics;intrinsic value (ontocentric);"All entities as informational clusters have minimal intrinsic moral worth; moral respect extends to informational entities.";IE is an environmental ethics based on information/infosphere/entropy rather than life/ecosystem/pain.;p.98‚Äì111;"Preserve data integrity and provenance; avoid needless deletion/distortion."
Ch.6 ‚Äî Respect and moral patients;respect as design stance;"Respect = appreciative, careful attention to the intrinsic value of patients; design should encode this stance.";A‚Äôs respect for P‚Äôs intrinsic value consists in‚Ä¶ appreciative and careful attention‚Ä¶ and a disposition to treat P appropriately.;p.113‚Äì114;Adopt ‚Äòrespect by default‚Äô: safe defaults, reversibility, user dignity in flows.
Ch.1 ‚Äî Re‚Äëontologized Infosphere;processors vs processed;"Digital tools and digital objects share the same ontology; interactions become uniformly digital and friction decreases.";There is no ontological difference between processors and processed‚Ä¶ interactions become equally digital.;p.6;"Expect rapid propagation; add throttles, rate limits, circuit breakers."
Ch.1 ‚Äî Zettabyte Age;risks & dilemmas;"ICTs bring benefits and dilemmas (identity theft, divides); ethics must address systemic risks.";ICTs also carry significant risks and generate dilemmas‚Ä¶;p.5;Include secondary-risk analysis (misuse, divides) in requirements.
Ch.12 ‚Äî Privacy & Ontological Friction;privacy as function of friction;Informational privacy depends on ontological friction that resists information flow.;Informational privacy is a function of the ontological friction in the infosphere.;p.231‚Äì232;Increase friction for sensitive flows (local-first, minimization, noise).
Ch.12 ‚Äî Privacy & Identity;identity construction;Privacy protects the freedom to (re)construct one‚Äôs identity over time.;Personal identity also depends on informational privacy.;p.243‚Äì247;"Support resets, deletion, pseudonyms; avoid sticky identifiers."
Ch.12 ‚Äî Biometrics & Labels;detachable identifiers;"Identity theft exploits detachable labels; robust identity uses ontic data with strict scope.";Arbitrary data‚Ä¶ merely associated with someone‚Äôs identity‚Ä¶ can easily be detached‚Ä¶ Enter biometrics.;p.247;"Minimize exposure of labels; bind identity narrowly with consent."
Ch.13 ‚Äî Moral Enablers vs Hinderers;steering DM;Better societies implement infraethics that supports the right DM and prevents moral hinderers.;An information society is a better society if it can implement an array of moral enablers‚Ä¶ while preventing‚Ä¶ moral hinderers.;p.275;Design incentives/guardrails that reduce harm and amplify beneficial coordination.
Ch.6 ‚Äî OOP Model of Moral Action;agent, patient, interactions;"Model moral action with components (agent, patient, interactions, frames of information); OOP is a convenient abstraction.";I shall model a moral action informationally‚Ä¶ relying on object‚Äëoriented programming‚Ä¶;p.114‚Äì116 (intro ¬ß6.1);Map system roles (agent/patient), interactions, and observables at design time.
Ch.13 ‚Äî Education & Governance;big issues ‚Üí big agents;The scale of ethical issues demands ‚Äòbig agents‚Äô: coordinated institutional governance for infraethics and DM.;Big issues call for big agents.;p.274‚Äì276;Adopt multi‚Äëstakeholder oversight and cross‚Äëteam accountability for AI.