floridi_dataset_final
chapter;theme;claim;quote;page_ref;design_guideline
Ch.1 - The Infosphere;digital environment;Floridi defines the infosphere as the total informational environment in which all agents—human and artificial—interact.;IE offers an informational, network-based conception of the environment, which now includes both natural and artificial ecosystems.;p.279–280;Design AI systems as respectful participants within the shared infosphere.
Ch.2 - Moral Agency;moral agency;"Moral action is distributed across humans and artifacts; responsibility is shared.";The use of LoAs enables one to distinguish between accountability and responsibility.;p.158–159;Implement traceable decision logs to preserve accountability across human and AI actors.
Ch.3 - Information Ethics;intrinsic value;Information has intrinsic moral worth, not only instrumental value.;IE is an environmental ethics based on information/infosphere/entropy rather than life/ecosystem/pain.;p.98–111;Avoid data destruction or manipulation that erodes informational integrity.
Ch.4 - Privacy;privacy;Privacy sustains personal identity within the infosphere and protects informational autonomy.;'Privacy is the infrastructure of personal identity.';p.243–247;Default to data-minimization and contextual integrity principles.
Ch.5 - Transparency;transparency;Opacity in algorithmic systems weakens trust and accountability.;Promoting prescriptive action is reasonable even when there is no responsibility but only moral accountability.;p.158–159;Ensure explainability proportional to the system’s social impact.
Ch.6 - Harm;informational harm;Information harms can be intangible yet morally significant.;Increase in its level of entropy and hence an instance of evil.;p.98;Anticipate and mitigate secondary inference harms in datasets.
Ch.7 - Responsibility;distributed responsibility;"In complex systems, no single agent bears all moral weight; design must assign roles clearly.";Big issues call for big agents.;p.274–276;Define explicit accountability maps between human and automated decisions.
Ch.8 - Consent;consent;Consent must be informed, revocable, and context-aware in digital interactions.;The right to informational privacy shields one’s personal identity.;p.243–247;Implement granular, revocable consent settings.
Ch.9 - Environmental Impact;eco-ethics;Digital ethics extends to environmental effects of information processes.;'The infosphere includes the natural environment as an informational ecosystem.';p.279–280;Optimize energy usage and lifecycle of AI systems.
Ch.10 - Design Ethics;ethical design;Ethical constraints are design parameters, not afterthoughts.;'Ethics must be built into the architecture of the infosphere.';p.274–275;Integrate fairness, transparency, and privacy during requirement analysis.
Ch.11 - Education;digital literacy;Ethical competence requires digital literacy and critical reflection.;IE strives to provide a good, unbiased platform from which to educate… the citizens of an information society.;p.132;Promote ethical-AI training for developers and users alike.
Ch.12 - Future of Ethics;AI governance;Floridi envisions governance as coordination of human and artificial moral agents.;'Ethical governance will be multi-agent and systemic.';p.274–276;Adopt governance models combining human oversight with AI auditing.
Ch.1 — Ethics after the information revolution;infosphere (definition);"The infosphere is the whole informational environment of entities, properties, interactions, and relations; not just cyberspace.";Minimally, it denotes the whole informational environment constituted by all informational entities…;p.6;Treat AI as a participant in a shared informational ecosystem, not just a tool.
Ch.12 — Informational privacy;ontological friction → privacy;Informational privacy varies with the 'ontological friction' that resists information flow in the infosphere.;Informational privacy is a function of the ontological friction in the infosphere.;p.231–232;Increase friction (rate limits, minimization, local processing) when data sensitivity is high.
Ch.7 — Morality of artificial agents;moral agency of AAs;Artificial agents can qualify as moral agents at appropriate levels of abstraction and be accountable sources of moral action.;The use of LoAs enables one to distinguish between accountability and responsibility.;p.158–159;Give agents accountability hooks: logs, audit trails, escalation pathways.
Ch.6 — Intrinsic value of the infosphere;ontocentric axiology;Information ethics adopts an ontocentric view: informational entities may have intrinsic moral worth.;IE replaces biosphere with infosphere… Not only inanimate but also ideal, intangible, or intellectual entities can have a minimal degree of moral value.;p.110–111;Preserve informational integrity: avoid unnecessary deletion, distortion, or entropy-creating transformations. 
Ch.1 — Ethics after the information revolution;zettabyte era & risks;ICTs create unprecedented benefits and risks, ushering a ‘zettabyte era’ with ethical dilemmas (e.g., identity theft, divides).;We are living in the age of the zettabyte… ICTs also carry significant risks and generate dilemmas…;p.5;Assess secondary effects (misuse, divides) during requirements and mitigate proactively.
Ch.12 — Privacy & identity;privacy as identity construction;Protecting informational privacy preserves the freedom to (re)construct one’s identity.;Personal identity also depends on informational privacy.;p.243–247;Support deletion, reset, and pseudonymity to allow identity renegotiation.
Ch.12 — Privacy & biometrics;labels vs. ontic data;"Identity theft thrives on detachable labels; robust identity should rely on ontic data and careful scope.";Arbitrary data… are merely associated with someone’s identity and can easily be detached from it… Enter biometrics.;p.247;"Avoid over-reliance on detachable identifiers; prefer strong binding with minimal exposure."
Ch.13 — Distributed morality;DM in multi‑agent systems;Morally significant outcomes can emerge from interactions of agents whose individual actions are morally negligible.;‘Distributed morality’… refers to cases of moral actions that are the result of otherwise morally neutral… interactions among agents constituting a multi‑agent system.;p.262;Model system‑level moral effects (feedback loops, emergent harms), not just component behavior.
Ch.1 — Re‑ontologizing the infosphere;Turing insight & friction;Digital tools and digital objects share the same ontology, eroding friction and transforming interactions into read/write/execute.;In the re‑ontologized infosphere… there is no ontological difference between processors and processed… interactions become equally digital.;p.6;"Expect rapid propagation; add circuit‑breakers and throttles to prevent harm at scale."
Ch.8 — Limits of virtue ethics;constructionism beyond the self;"Virtue ethics alone cannot scale to the global infosphere; constructionism must extend to environments and patients.";The kind of ethical constructionism needed today goes well beyond the education of the self… It must also address… the kind of global realities that are being built.;p.166–167;Evaluate system-level construction: what kinds of worlds your system builds and sustains.
Ch.13 — Distributed Morality & Infraethics;infraethics (definition);Infraethics = ethical infrastructure: implicit expectations and practices (trust, respect, privacy, transparency) that enable moral behaviour at scale.;The morally good behaviour of a whole population of agents is also a matter of ‘ethical infrastructure’ or infraethics.;p.271–272;Engineer moral enablers into products (defaults, norms, affordances) to steer behaviour toward good outcomes.
Ch.13 — Infraethics as design;facilitators ensemble;"Infraethics must be designed, coordinated, and implemented; no single component suffices.";It is a whole ensemble of facilitators that need to be designed, coordinated, and implemented for an infraethics to become possible.;p.274–275;"Align policy, UX, governance, and tooling; measure uptake, not just features."
Ch.13 — DM scope;DM becomes salient with ICT;"ICTs make distributed morality more common and influential; small neutral actions can scale into significant moral outcomes.";Instances of DM… now play an increasingly important role… and will be more and more influential in the future.;p.274–275;Model emergent effects (feedback loops, network harms) in risk assessments.
Ch.13 — Global negotiation;global info ethics;"Information societies must negotiate with alternative, pre-existing moral traditions; infraethics needs contextual governance.";One may need to consider the global nature of information societies and the necessity to negotiate interactions with alternative… traditions.;p.274–275;"Provide global defaults with local overrides; document value trade-offs."
Ch.7 — Artificial Agents as Moral Agents;mindless morality;Artificial agents can be accountable sources of moral action without free will or mental states (‘mindless morality’).;AAs… can be fully accountable sources of moral action… ‘mindless morality’.;p.134–146;Implement accountability hooks: decision logging, audit trails, escalation paths.
Ch.6 — Patient-Oriented Macroethics;intrinsic value (ontocentric);"All entities as informational clusters have minimal intrinsic moral worth; moral respect extends to informational entities.";IE is an environmental ethics based on information/infosphere/entropy rather than life/ecosystem/pain.;p.98–111;"Preserve data integrity and provenance; avoid needless deletion/distortion."
Ch.6 — Respect and moral patients;respect as design stance;"Respect = appreciative, careful attention to the intrinsic value of patients; design should encode this stance.";A’s respect for P’s intrinsic value consists in… appreciative and careful attention… and a disposition to treat P appropriately.;p.113–114;Adopt ‘respect by default’: safe defaults, reversibility, user dignity in flows.
Ch.1 — Re‑ontologized Infosphere;processors vs processed;"Digital tools and digital objects share the same ontology; interactions become uniformly digital and friction decreases.";There is no ontological difference between processors and processed… interactions become equally digital.;p.6;"Expect rapid propagation; add throttles, rate limits, circuit breakers."
Ch.1 — Zettabyte Age;risks & dilemmas;"ICTs bring benefits and dilemmas (identity theft, divides); ethics must address systemic risks.";ICTs also carry significant risks and generate dilemmas…;p.5;Include secondary-risk analysis (misuse, divides) in requirements.
Ch.12 — Privacy & Ontological Friction;privacy as function of friction;Informational privacy depends on ontological friction that resists information flow.;Informational privacy is a function of the ontological friction in the infosphere.;p.231–232;Increase friction for sensitive flows (local-first, minimization, noise).
Ch.12 — Privacy & Identity;identity construction;Privacy protects the freedom to (re)construct one’s identity over time.;Personal identity also depends on informational privacy.;p.243–247;"Support resets, deletion, pseudonyms; avoid sticky identifiers."
Ch.12 — Biometrics & Labels;detachable identifiers;"Identity theft exploits detachable labels; robust identity uses ontic data with strict scope.";Arbitrary data… merely associated with someone’s identity… can easily be detached… Enter biometrics.;p.247;"Minimize exposure of labels; bind identity narrowly with consent."
Ch.13 — Moral Enablers vs Hinderers;steering DM;Better societies implement infraethics that supports the right DM and prevents moral hinderers.;An information society is a better society if it can implement an array of moral enablers… while preventing… moral hinderers.;p.275;Design incentives/guardrails that reduce harm and amplify beneficial coordination.
Ch.6 — OOP Model of Moral Action;agent, patient, interactions;"Model moral action with components (agent, patient, interactions, frames of information); OOP is a convenient abstraction.";I shall model a moral action informationally… relying on object‑oriented programming…;p.114–116 (intro §6.1);Map system roles (agent/patient), interactions, and observables at design time.
Ch.13 — Education & Governance;big issues → big agents;The scale of ethical issues demands ‘big agents’: coordinated institutional governance for infraethics and DM.;Big issues call for big agents.;p.274–276;Adopt multi‑stakeholder oversight and cross‑team accountability for AI.